import time
import os
import json
import cv2
# import re
# import torch
import numpy as np
import pandas as pd
import tensorflow as tf
# import supervision as sv
# import matplotlib.pyplot as plt
# import matplotlib.patches as patches
# from matplotlib.patches import Patch
from ultralytics import YOLO
from PIL import Image, ImageDraw, ImageFont
from paddleocr import PaddleOCR, draw_ocr
from pdf2image import convert_from_path
from google import genai
from google.genai import types
from unidecode import unidecode
from fuzzywuzzy import fuzz
from datetime import datetime
# import base64
# import tempfile

from core.config import MODEL_SIGNATURE_PATH, MODEL_TABLE_TITLE_PATH, DEVICE, POPPLER_PATH, financial_tables, models, EXTRACTED_FOLDER
from utils import retry_api_call, dataframe_to_json, json_to_dataframe
from secret import api_keys


class TableDetectService:
    def __init__(self):
        print("TableDetectService: Kh·ªüi t·∫°o l√† load model...")
        self.ocr = PaddleOCR(lang="en")
        self.table_title_detection_model = YOLO(MODEL_TABLE_TITLE_PATH).to(DEVICE)
        self.signature_detection_model = YOLO(MODEL_SIGNATURE_PATH).to(DEVICE)
        self.detection_class_names = ["table", "table rotated"]
        self.structure_class_map = {
            k: v
            for v, k in enumerate(
                [
                    "table",
                    "table column",
                    "table row",
                    "table column header",
                    "table projected row header",
                    "table spanning cell",
                    "no object",
                ]
            )
        }
        self.structure_class_thresholds = {
            "table": 0.5,
            "table column": 0.5,
            "table row": 0.5,
            "table column header": 0.5,
            "table projected row header": 0.5,
            "table spanning cell": 0.5,
            "no object": 10,  # Gi√° tr·ªã cao ƒë·ªÉ lo·∫°i b·ªè "no object"
        }

    def detect_table(self, pdf_path, file_name_origin):
        """
        Flow x·ª≠ l√Ω file pdf nh∆∞ sau:
        - Chuy·ªÉn file pdf th√†nh danh s√°ch c√°c h√¨nh ·∫£nh
        - L·∫∑p qua t·ª´ng h√¨nh v√† x·ª≠ l√Ω nh∆∞ sau:
            + N·∫øu h√¨nh ƒë√≥ c√≥ ch·ª©a table:
                > S·ª≠ d·ª•ng model best_model_YOlO ƒë·ªÉ detect ra b·∫≥ng
        """
        recognized_titles_set = set() # bi·∫øn n√†y ƒë·ªÉ t·∫°o 1 set l∆∞u tr·ªØ nh·ªØng b·∫£ng m√¨nh ƒë√£ tr√≠ch xu·∫•t, khi n√†o ƒë·ªß 3 b·∫£ng r·ªìi th√¨ d·ª´ng ch∆∞∆°ng t√¨nh
        dfs_dict = {} # Bi·∫øn n√†y ƒë·ªÉ concat d·ªØ li·ªáu t·ª´ng b·∫£ng, v√¨ b·∫£ng c√≥ th·ªÉ d√†i qu√° b·ªã chuy·ªÉn sang page kh√°c


        images = self.pdf_to_images(pdf_path)  # Chuy·ªÉn pdf th√†nh h√¨nh ·∫£nh

        index_start = 0  # B·∫Øt ƒë·∫ßu t·ª´ ·∫£nh ƒë·∫ßu ti√™n
        while index_start < len(images):
            index_chuky = None  # Reset m·ªói l·∫ßn l·∫∑p 
            for i in range(index_start, len(images)):
                selected_images = [] #T·∫°o 1 list ƒë·ªÉ l∆∞u c√°c b·∫£ng chung 1 title ƒëi t·ª´ title ƒë·∫øn ·∫£nh ch·ªØ k√Ω ƒë·∫ßu ti√™n nh·∫≠n di·ªán ƒë∆∞·ª£c
                image = images[i]
                print(f"üîç ƒêang x·ª≠ l√Ω ·∫£nh {i+1}")

                # Nh·∫≠n di·ªán b·∫£ng -> table-title
                nhandien_table = self.table_detection(image)

                if not nhandien_table:
                    continue  # N·∫øu kh√¥ng c√≥ b·∫£ng, b·ªè qua ·∫£nh n√†y

                has_rotated_table = any(
                    self.detection_class_names[det[5]] == "table rotated"
                    for det in nhandien_table
                )
                
                # Ch·ªâ xoay ·∫£nh n·∫øu c√≥ b·∫£ng xoay
                image_to_process = (
                    self.table_rotation(image, nhandien_table) if has_rotated_table else image
                )

                # Nh·∫≠n di·ªán ti√™u ƒë·ªÅ --> ph√¢n v√¢n
                # df_title, text_title = self.detect_and_extract_title(
                #     image_to_process,
                #     "/content/drive/MyDrive/Test AI oÃõÃâ Orient/AI_for_Finance/BaÃân sao cuÃâa best_model_YoLo.pt",
                #     ocr,
                # )

                df_title, text_title = self.detect_and_extract_title(image_to_process) # h√†m nh·∫≠n di·ªán c√°c text ngo√†i b·∫£ng v√† tr·∫£ v·ªÅ 1 DataFrame v√† 1 bi·∫øn l∆∞u text ƒë·ªÉ cho LLM nh·∫≠n ng·ªØ c·∫£nh

                # ƒê·ªÉ sleep ƒë·ªÉ gi√∫p model ngh·ªâ, b·ªã limit 1 ph√∫t kh√¥ng qu√° 2 l·∫ßn
                time.sleep(45)

                for model in models:
                    temperature, top_p, top_k = self.get_model_params(model) # Setup model, hi·ªán t·∫°i ƒëang d√πng 2 model LLM t·ª´ng model t·ª´ng tham s·ªë
                    for api_key in api_keys:
                        json_title = retry_api_call(         #H√†m n√†y ƒë·ªÉ thay API lu√¢n phi√™n
                            self.generate_title,            # H√†m n√†y ƒë·ªÉ d√πng LLM chu·∫©n h√≥a l·∫°i d·ªØ li·ªáu ti·∫øng Vi·ªát cho c√°c Text ngo√†i b·∫£ng
                            model,
                            api_keys[api_key]["title"],
                            temperature,
                            top_p,
                            top_k,
                            dataframe_to_json(df_title),
                            text_title,
                        )
                        if json_title:
                            break
                    if json_title:
                        break
                print("Ho√†n t·∫•t th·ª≠ API.")

                data_title = json_to_dataframe(json_title)  # K·∫øt qu·∫£ title c·ªßa b·∫£ng
                recognized_title = self.recognize_financial_table(
                    data_title, financial_tables, threshold=80
                )  # Nh·∫≠n di·ªán xem title c·ªßa b·∫£ng l√† g√¨ c√≥ ph√π h·ª£p v·ªõi 3 t√™n b·∫£ng d·ª± √°n ƒë·ªÅ ra kh√¥ng

                # N·∫øu nh·∫≠n di·ªán ƒë∆∞·ª£c title, th√™m v√†o danh s√°ch nh·∫≠n di·ªán
                if not (recognized_title):
                    continue
                # T√¨m ·∫£nh ch·ªØ k√Ω ti·∫øp theo sau ·∫£nh title
                for j in range(images.index(image), len(images)):
                    nhandien_chuky = images[j]
                    results_chuky = self.detect_signature(nhandien_chuky)
                    if results_chuky[0]:
                        index_chuky = j  # L∆∞u v·ªã tr√≠ ·∫£nh ch·ªØ k√Ω
                        print(f"üñä ·∫¢nh ch·ªØ k√Ω ƒë∆∞·ª£c ph√°t hi·ªán ·ªü {index_chuky +1 }")
                        break

                # L·∫•y danh s√°ch ·∫£nh t·ª´ title ƒë·∫øn ch·ªØ k√Ω
                if index_chuky:
                    selected_images.extend(images[images.index(image) : index_chuky + 1])

                # V√≤ng l·∫∑p qua ·∫£nh t·ª´ title ƒë·∫øn ch·ªØ k√Ω ƒë·ªÉ tr√≠ch xu·∫•t b·∫£ng
                if selected_images:
                    pre_name_column = None
                    for img in selected_images:
                        processed_image = self.Process_Image(img) # H√†m n√†y nh·∫≠n di·ªán b·∫£ng trong page r·ªìi c·∫Øt b·∫£ng, xoay b·∫£ng n·∫øu c√≥
                        if processed_image is not None:
                            df_table, text_table = self.process_pdf_image(processed_image) # H√†m n√†y d√πng OCR ƒë·ªÉ tr√≠ch xu·∫•t th√¥ng tin trong b·∫£ng, tr·∫£ v·ªÅ 1 DataFrame v√† 1 text ƒë·ªÉ gi√∫p LLM hi·ªÉu ng·ªØ nghƒ©a
                            if not df_table.empty:    #C√°i n√†y ƒë·ªÉ ƒëi·ªÅu ch·ªânh token, √≠t token qu√° th√¨ LLM kh tr·∫£ ƒë·ªß d·ªØ li·ªáu, nhi·ªÅu qu√° th√¨ nhanh t·ªën
                                if (len(df_table) < 101) and (len(df_table.columns) < 10):
                                    token = 9000
                                elif (len(df_table) < 201) and (len(df_table.columns) < 10):
                                    token = 18000
                                else:
                                    token = 30000
                                time.sleep(45)
                                for model in models:
                                    temperature, top_p, top_k = self.get_model_params(model)
                                    for api_key in api_keys:
                                        json_table = retry_api_call(    #H√†m n√†y ƒë·ªÉ thay lu√¢n phi√™n API key
                                            self.generate_table,            #D√πng LLM ƒë·ªÉ chu·∫©n h√≥a, fix l·∫°i d·ªØ li·ªáu ƒë·ªçc t·ª´ b·∫£ng
                                            model,
                                            api_keys[api_key]["table"],
                                            temperature,
                                            top_p,
                                            top_k,
                                            dataframe_to_json(df_table),
                                            text_table,
                                            token,
                                            pre_name_column,
                                        )
                                        if json_table:
                                            break
                                    if json_table:
                                        break
                                print("Ho√†n t·∫•t th·ª≠ API.")

                                data_table = json_to_dataframe(json_table)

                                found = False  # Flag ƒë·ªÉ tho√°t c·∫£ hai v√≤ng l·∫∑p khi t√¨m th·∫•y k·∫øt qu·∫£
                                for column in data_table.columns:   # ·ªû ph·∫ßn n√†y ƒë·ªÉ gi√∫p chu·∫©n h√≥a l·∫°i t√™n b·∫£ng, gi√∫p g·ªôp d·ªØ li·ªáu chu·∫©n h∆°n
                                    for value in data_table[column].dropna(): #C√≥ 1 s·ªë b√°o c√°o t√†i ch√≠nh b·ªã sai t√™n b·∫£ng
                                        value = self.normalize_text(value)

                                        if "luu chuyen" in value:
                                            recognized_title = "B√°o c√°o l∆∞u chuy·ªÉn ti·ªÅn t·ªá"
                                            found = True
                                            break  # Tho√°t kh·ªèi v√≤ng l·∫∑p gi√° tr·ªã trong c·ªôt

                                        if (
                                            "doanh thu ban hang" in value
                                            or "ban hang" in value
                                        ):
                                            recognized_title = (
                                                "B√°o c√°o k·∫øt qu·∫£ ho·∫°t ƒë·ªông kinh doanh"
                                            )
                                            found = True
                                            break  # Tho√°t kh·ªèi v√≤ng l·∫∑p gi√° tr·ªã trong c·ªôt

                                    if found:
                                        break  # Tho√°t kh·ªèi v√≤ng l·∫∑p c·ªôt

                                print(f"Fix nh·∫≠n di·ªán ƒë∆∞·ª£c l√† {recognized_title}")

                                recognized_titles_set.add(recognized_title) #L∆∞u n√≥ v√†o set ƒë√£ t·∫°o ·ªü tr∆∞·ªõc, gi√∫p nh·∫≠n di·ªán n√†o ƒë·ªß 3 b·∫£ng th√¨ d·ª´ng l·∫°i
                                # display(data_table)
                                if selected_images.index(img) == 0:
                                    pre_name_column = data_table.columns.tolist() # V√¨ model LLM hay tr·∫£ k·∫øt qu·∫£ v·ªÅ 1 l√∫c 1 kh√°c n√™n d√πng t√™n c√°c c·ªôt c·ªßa b·∫£ng ƒë·∫ßu ti√™n l√†m chu·∫©n ƒë·ªÉ c√°c b·∫£ng sau tr·∫£ v·ªÅ cho chu·∫©n
                                else:
                                    if len(data_table.columns) == len(pre_name_column):
                                        data_table.columns = pre_name_column
                                    else:
                                        data_table = data_table.reindex(
                                            columns=pre_name_column, fill_value=None
                                        )

                                if not data_table.empty:                            #·ªû ƒë√¢y ki·ªÉm tra trong bi·∫øn dfs_dict ƒë√£ c√≥ DataFrame v·ªõi key l√† tiitle ƒëang nh·∫≠n di·ªán ch∆∞a, n·∫øu ch∆∞a th√¨ l∆∞u v√†o bi·∫øn dfs_dict DataFrame v·ªõi key ƒë·∫•y
                                    if recognized_title not in dfs_dict:            # N·∫øu ·ªü trong dfs_dict ƒë√£ c√≥ DataFrame v·ªõi key l√† titile ƒëang nh·∫≠n di·ªán th√¨ n√≥ s·∫Ω n·ªëi (concat d·ªØ li·ªáu l·∫°i d·ª±a tr√™n key l√† title ƒë·∫•y)
                                        dfs_dict[recognized_title] = data_table
                                    else:
                                        dfs_dict[recognized_title] = pd.concat(
                                            [dfs_dict[recognized_title], data_table],
                                            ignore_index=True,
                                        )
                    # display(dfs_dict[recognized_title])

                    break # beak ƒë·ªÉ c·∫≠p nh·∫≠t l·∫°i v√≠ tr√≠ b·∫Øt ƒë·∫ßu l√† 
                
            # C·∫≠p nh·∫≠t v·ªã tr√≠ b·∫Øt ƒë·∫ßu cho v√≤ng l·∫∑p ti·∫øp theo. V√¨ c√°ch ch·∫°y l√† v√≠ d·ª• b·∫£ng C√¢n ƒë·ªëi k·∫ø to√°n ·ªü tr∆∞·ªõc
            # Th√¨ x√°c ƒë·ªãnh ƒë∆∞·ª£c index_chuky c·ªßa b·∫£ng C√¢n ƒë·ªëi k·∫ø to√°n r·ªìi th√¨ update l·∫°i l√™n v√≤ng l·∫∑p cho n√≥ ch·∫°y t·ª´ ch·ªØ k√Ω ch·∫°y ti·∫øp.
            if index_chuky:        
                index_start = index_chuky + 1
            else:
                index_start = i + 1
                # Ki·ªÉm tra n·∫øu ƒë√£ nh·∫≠n di·ªán ƒë·ªß b·∫£ng t√†i ch√≠nh th√¨ d·ª´ng
            if recognized_titles_set == set(financial_tables):
                print("‚úÖ ƒê√£ nh·∫≠n di·ªán ƒë·ªß t·∫•t c·∫£ b·∫£ng t√†i ch√≠nh. D·ª´ng l·∫°i!")
                break

        # L∆∞u k·∫øt qu·∫£ v√†o file Excel
        name, _ = file_name_origin.rsplit(".", 1) if "." in file_name_origin else (file_name_origin, "")
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")  # ƒê·ªãnh d·∫°ng th·ªùi gian: YYYYMMDD_HHMMSS
        new_name = f"{name}_{timestamp}.xlsx"
        file_path = os.path.join(EXTRACTED_FOLDER, new_name)
        with pd.ExcelWriter(file_path, engine="xlsxwriter") as writer: # TODO: No module named 'xlsxwriter'
            for i, (sheet_name, df) in enumerate(dfs_dict.items()):
                df.to_excel(writer, sheet_name=sheet_name[:31], index=False)

                # N·∫øu kh√¥ng ph·∫£i l·∫ßn cu·ªëi c√πng, th√¨ ch·ªù tr∆∞·ªõc khi g·ª≠i request ti·∫øp theo
                if i < len(dfs_dict) - 1:
                    print(f"Ch·ªù 30 gi√¢y tr∆∞·ªõc khi ti·∫øp t·ª•c l∆∞u b·∫£ng ti·∫øp theo...")
                    time.sleep(30)  # Ch·ªù 30 gi√¢y gi·ªØa c√°c request

        print(f"File Excel ƒë√£ ƒë∆∞·ª£c l∆∞u t·∫°i: {file_path}")
        download_url = f"/{EXTRACTED_FOLDER}/{new_name}"

        return dfs_dict, download_url
    
        
    def pdf_to_images(self, pdf_path):
        images = convert_from_path(pdf_path, poppler_path=POPPLER_PATH)
        return images

    # model table_title_detection_model
    def table_detection(self, image):
        imgsz = 800
        pred = self.table_title_detection_model.predict(image, imgsz=imgsz)
        pred = pred[0].boxes
        result = pred.cpu().numpy()
        result_list = [
            list(result.xywhn[i]) + [result.conf[i], int(result.cls[i])]
            for i in range(result.shape[0])
        ]
        return result_list

    def table_rotation(self, image, list_detection_table):
        for det in list_detection_table:
            x_center, y_center, w_n, h_n, conf, cls_id = det
            if self.detection_class_names[cls_id] == "table rotated":
                print("This is a rotated table")
                image = image.rotate(-90, expand=True)
            img = image.convert("L")
            thresh_img = img.point(lambda p: 255 if p > 120 else 0)
            return thresh_img

    # model table_title_detection_model
    def Process_Image(self, image):
        results = self.table_title_detection_model.predict(image, task="detect")
        boxes = results[0].boxes

        for box in boxes:
            x1, y1, x2, y2 = box.xyxy[0].tolist()
            cls_id = int(box.cls[0])
            class_name = self.table_title_detection_model.names[cls_id]

            # Chuy·ªÉn ƒë·ªïi sang PIL n·∫øu c·∫ßn
            if isinstance(image, np.ndarray):
                image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))

            # C·∫Øt ·∫£nh tr∆∞·ªõc
            cropped_table = image.crop((int(x1), int(y1), int(x2), int(y2)))

            # plt.imshow(cropped_table)
            # plt.title("Cropped Image")
            # plt.show()

            # N·∫øu l√† b·∫£ng b·ªã xoay, xoay l·∫°i
            if class_name == "table rotated":
                print("This is a rotated table")
                cropped_img = cropped_table.rotate(-90, expand=True)
                return cropped_img  # Tr·∫£ v·ªÅ ·∫£nh ƒë√£ c·∫Øt v√† s·ª≠a g√≥c

            return cropped_table  # N·∫øu kh√¥ng b·ªã xoay, tr·∫£ v·ªÅ ·∫£nh c·∫Øt nguy√™n b·∫£n

    # s·ª≠ d·ª•ng model t·ª´ models = ["gemini-2.0-pro-exp-02-05", "gemini-2.0-flash-thinking-exp-01-21"]
    # chuy·ªÉn API key sang file config
    def generate_title(self, model, API, temperature, top_p, top_k, path_title_json, text_title):
        result = ""
        client = genai.Client(api_key=f"{API}")

        # M·ªü file JSON v√† ƒë·ªçc n·ªôi dung
        file_path = path_title_json
        with open(file_path, "r", encoding="utf-8") as f:
            json_content = json.load(f)  # Load JSON th√†nh dict

        model = model
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(
                        text=f"""M√¨nh ƒëang tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ h√¨nh ·∫£nh ch·ª©a b·∫£ng t√†i ch√≠nh b·∫±ng PaddleOCR. D·ªØ li·ªáu nh·∫≠n di·ªán ƒë∆∞·ª£c l∆∞u trong {text_title}.
    Tuy nhi√™n, d·ªØ li·ªáu g·∫∑p l·ªói:
    - Sai ch√≠nh t·∫£ ti·∫øng Vi·ªát trong b√°o c√°o t√†i ch√≠nh, k·∫ø to√°n v√† d√≤ng ti·ªÅn
    - L·ªói ng·ªØ ph√°p ti·∫øng Vi·ªát trong b√°o c√°o t√†i ch√≠nh, k·∫ø to√°n v√† d√≤ng ti·ªÅn
    V√¨ ƒë√¢y l√† m·ªôt b√°o c√°o quan tr·ªçng, r·∫•t nhi·ªÅu th·ª© ·∫£nh h∆∞·ªüng x·∫•u ƒë·∫øn n·∫øu nh∆∞ n√≥ sai ch√≠nh t·∫£ v√† l·ªói ng·ªØ ph√°p.
    B·∫°n h√£y tr·∫£ v·ªÅ cho m√¨nh m·ªôt DataFrame ch·ªâ c√≥ 1 c·ªôt l√† "values" ch·ª©a c√°c gi√° tr·ªã ƒë∆∞·ª£c ngƒÉn c√°ch th√†nh t·ª´ng d√≤ng gi√∫p ng∆∞·ªùi ƒë·ªçc d·ªÖ d√†ng ƒë·ªçc hi√™u, m·ªói h√†ng kh√¥ng ch·ª©a l·ªìng gh√©p th√†nh chu·ªói hay danh s√°ch g√¨, ch·ªâ 1 d√≤ng l√† 1 gi√° tr·ªã ri√™ng bi·ªát t·ª´ file JSON g·ªëc.
                        D·ªØ li·ªáu JSON g·ªëc:
                        {json.dumps(json_content, indent=2, ensure_ascii=False)}
                        """
                    ),
                ],
            ),
        ]

        generate_content_config = types.GenerateContentConfig(
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            max_output_tokens=8192,
            response_mime_type="application/json",
        )

        for chunk in client.models.generate_content_stream(
            model=model,
            contents=contents,
            config=generate_content_config,
        ):
            result += chunk.text
        return result

    # s·ª≠ d·ª•ng model t·ª´ models = ["gemini-2.0-pro-exp-02-05", "gemini-2.0-flash-thinking-exp-01-21"]
    # chuy·ªÉn API key sang file config
    def generate_table(self, model, API, temperature, top_p, top_k, path_dataframe_json, text_table, token, table_columns):
        result = ""
        client = genai.Client(api_key=f"{API}")  # ƒêu√¥i nc

        # M·ªü file JSON v√† ƒë·ªçc n·ªôi dung
        file_path = path_dataframe_json
        with open(file_path, "r", encoding="utf-8") as f:
            json_content = json.load(f)  # Load JSON th√†nh dict

        model = model
        contents = [
            types.Content(
                role="user",
                parts=[
                    types.Part.from_text(
                        text=f"""M√¨nh ƒëang tr√≠ch xu·∫•t d·ªØ li·ªáu t·ª´ h√¨nh ·∫£nh ch·ª©a b·∫£ng t√†i ch√≠nh b·∫±ng PaddleOCR. D·ªØ li·ªáu nh·∫≠n di·ªán ƒë∆∞·ª£c l∆∞u trong {text_table}.
    Tuy nhi√™n, d·ªØ li·ªáu g·∫∑p l·ªói:
    - Sai ch√≠nh t·∫£ ti·∫øng Vi·ªát
    - L·ªói ng·ªØ ph√°p ti·∫øng Vi·ªát
    - S·∫Øp x·∫øp sai d√≤ng/c·ªôt, ·∫£nh h∆∞·ªüng ƒë·∫øn t√≠nh ch√≠nh x√°c c·ªßa b√°o c√°o t√†i ch√≠nh.

    B·∫°n h√£y gi√∫p m√¨nh chu·∫©n h√≥a l·∫°i b·∫£ng d·ªØ li·ªáu d·ª±a v√†o b·ªëi c·∫£nh v√† k√Ω t·ª± nh·∫≠n di·ªán ƒë∆∞·ª£c trong {text_table} v√† ki·∫øn th·ª©c chuy√™n ng√†nh t√†i ch√≠nh, k·∫ø to√°n, ƒë·∫£m b·∫£o ƒë√∫ng thu·∫≠t ng·ªØ, ch√≠nh t·∫£ v√† c·∫•u tr√∫c b·∫£ng h·ª£p l√Ω (g·ªìm d√≤ng, c·ªôt, ti√™u ƒë·ªÅ c·ªôt, d·ªØ li·ªáu trong b·∫£ng). K·∫øt qu·∫£ tr·∫£ v·ªÅ l√† m·ªôt DataFrame chu·∫©n theo ƒë√∫ng ƒë·ªãnh d·∫°ng b·∫£ng b√°o c√°o t√†i ch√≠nh cho ng∆∞·ªùi d√πng d·ªÖ d√†ng ƒë·ªçc hi·ªÉu,
    ƒë·∫£m b·∫£o ƒë√∫ng th√¥ng tin ƒë∆∞·ª£c truy·ªÅn v√†o t·ª´ bi·∫øn {text_table} v√† D·ªØ li·ªáu JSON g·ªëc kh√¥ng sai k·∫øt qu·∫£.
    ƒê√¢y l√† b√°o c√°o k·∫øt qu·∫£ ho·∫°t ƒë·ªông kinh doanh c·ªßa c√¥ng ty ABC.
    B·∫°n h√£y ki·ªÉm tra n·∫øu danh s√°ch t√™n c·ªôt {table_columns} r·ªóng th√¨ h√£y nh·∫≠n di·ªán ƒë·ªÉ ƒë·∫∑t t√™n c·ªôt m·∫∑c ƒë·ªãnh b·∫Øt bu·ªôc ph·∫£i c√≥ ch·ª©a 3 c·ªôt: "M√£ s·ªë", "T√™n ch·ªâ ti√™u", "Thuy·∫øt minh" v√† chu·∫©n h√≥a c√°c c·ªôt sau: "M√£ s·ªë", "T√™n ch·ªâ ti√™u", "Thuy·∫øt minh".
    N·∫øu danh s√°ch t√™n c·ªôt {table_columns} kh√¥ng r·ªóng th√¨ h√£y ƒë·∫∑t t√™n c·ªôt gi·ªëng nh∆∞ t·ª´ng gi√° tr·ªã trong {table_columns} v√† chu·∫©n h√≥a ch√∫ng ƒë√∫ng v·ªõi ki·∫øn th·ª©c quan tr·ªçng c·∫ßn thi·∫øt trong b√°o c√°o t√†i ch√≠nh.
    T·ª± ƒë·ªông nh·∫≠n di·ªán v√† chu·∫©n h√≥a c√°c c·ªôt s·ªë li·ªáu, ƒë·∫£m b·∫£o ch√∫ng ƒë∆∞·ª£c hi·ªÉn th·ªã ƒë√∫ng ƒë·ªãnh d·∫°ng (v√≠ d·ª•: s·ªë nguy√™n, s·ªë th·∫≠p ph√¢n, ƒë∆°n v·ªã ti·ªÅn t·ªá).
    N·∫øu c√≥ th·ªÉ, h√£y x√°c ƒë·ªãnh nƒÉm t√†i ch√≠nh ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p trong b√°o c√°o v√† s·ª≠ d·ª•ng th√¥ng tin n√†y ƒë·ªÉ ƒë·∫∑t t√™n cho c√°c c·ªôt s·ªë li·ªáu (v√≠ d·ª•: "NƒÉm 2022", "NƒÉm 2023").
    S·ª≠ d·ª•ng t√™n c·ªôt c√≥ d·∫•u c√°ch v√† vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu ti√™n c·ªßa m·ªói t·ª´.

    D·ªØ li·ªáu JSON g·ªëc:
    {json.dumps(json_content, indent=2, ensure_ascii=False)}
    """
                    ),
                ],
            ),
        ]

        generate_content_config = types.GenerateContentConfig(
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            max_output_tokens=token,
            response_mime_type="application/json",
        )

        for chunk in client.models.generate_content_stream(
            model=model,
            contents=contents,
            config=generate_content_config,
        ):
            result += chunk.text
        return result

    def process_image_ocr(self, image):
        """Nh·∫≠n di·ªán text trong ·∫£nh b·∫±ng OCR."""
        if isinstance(image, Image.Image):
            image = np.array(image)
        output = self.ocr.ocr(image)[0]
        boxes = [line[0] for line in output]
        texts = [line[1][0] for line in output]
        probabilities = [line[1][1] for line in output]
        return image, boxes, texts, probabilities

    def get_horizontal_vertical_boxes(self, image, boxes):
        """T·∫°o danh s√°ch c√°c bounding box ngang v√† d·ªçc."""
        image_height, image_width = image.shape[:2]
        horiz_boxes = []
        vert_boxes = []

        for box in boxes:
            x_h, x_v = 0, int(box[0][0])
            y_h, y_v = int(box[0][1]), 0
            width_h, width_v = image_width, int(box[2][0] - box[0][0])
            height_h, height_v = int(box[2][1] - box[0][1]), image_height

            horiz_boxes.append([x_h, y_h, x_h + width_h, y_h + height_h])
            vert_boxes.append([x_v, y_v, x_v + width_v, y_v + height_v])

        return horiz_boxes, vert_boxes

    def apply_non_max_suppression(self, boxes, scores, image):
        """√Åp d·ª•ng Non-Max Suppression (NMS) ƒë·ªÉ lo·∫°i b·ªè c√°c bounding box d∆∞ th·ª´a."""
        nms_indices = tf.image.non_max_suppression(
            boxes,
            scores,
            max_output_size=1000,
            iou_threshold=0.1,
            score_threshold=float("-inf"),
        ).numpy()
        return np.sort(nms_indices)

    def intersection(self, box_1, box_2):
        """T√≠nh to√°n giao gi·ªØa hai bbox."""
        return [box_2[0], box_1[1], box_2[2], box_1[3]]

    def iou(self, box_1, box_2):
        """T√≠nh ch·ªâ s·ªë Intersection over Union (IoU)."""
        x_1, y_1 = max(box_1[0], box_2[0]), max(box_1[1], box_2[1])
        x_2, y_2 = min(box_1[2], box_2[2]), min(box_1[3], box_2[3])
        inter = abs(max((x_2 - x_1, 0)) * max((y_2 - y_1), 0))

        if inter == 0:
            return 0
        box_1_area = abs((box_1[2] - box_1[0]) * (box_1[3] - box_1[1]))
        box_2_area = abs((box_2[2] - box_2[0]) * (box_2[3] - box_2[1]))

        return inter / float(box_1_area + box_2_area - inter)

    def extract_table_data(self, boxes, texts, horiz_lines, vert_lines, horiz_boxes, vert_boxes):
        """Tr√≠ch xu·∫•t d·ªØ li·ªáu b·∫£ng t·ª´ bbox ƒë√£ nh·∫≠n di·ªán."""
        out_array = [["" for _ in range(len(vert_lines))] for _ in range(len(horiz_lines))]

        unordered_boxes = [vert_boxes[i][0] for i in vert_lines]
        ordered_boxes = np.argsort(unordered_boxes)

        for i in range(len(horiz_lines)):
            for j in range(len(vert_lines)):
                resultant = self.intersection(
                    horiz_boxes[horiz_lines[i]], vert_boxes[vert_lines[ordered_boxes[j]]]
                )
                for b, box in enumerate(boxes):
                    the_box = [box[0][0], box[0][1], box[2][0], box[2][1]]
                    if self.iou(resultant, the_box) > 0.1:
                        out_array[i][j] = texts[b]

        return pd.DataFrame(np.array(out_array))

    def process_pdf_image(self, image):
        """H√†m t·ªïng h·ª£p ƒë·ªÉ x·ª≠ l√Ω ·∫£nh t·ª´ PDF, nh·∫≠n di·ªán b·∫£ng v√† tr√≠ch xu·∫•t d·ªØ li·ªáu."""
        # OCR tr√≠ch xu·∫•t text & bbox
        image, boxes, texts, probabilities = self.process_image_ocr(image)

        # Nh·∫≠n di·ªán box ngang & d·ªçc
        horiz_boxes, vert_boxes = self.get_horizontal_vertical_boxes(image, boxes)

        # Lo·∫°i b·ªè c√°c box d∆∞ th·ª´a b·∫±ng Non-Max Suppression
        horiz_lines = self.apply_non_max_suppression(horiz_boxes, probabilities, image)
        vert_lines = self.apply_non_max_suppression(vert_boxes, probabilities, image)

        # Tr√≠ch xu·∫•t d·ªØ li·ªáu b·∫£ng th√†nh DataFrame
        df = self.extract_table_data(
            boxes, texts, horiz_lines, vert_lines, horiz_boxes, vert_boxes
        )

        return df, texts

    # model nh·∫≠n di·ªán ch·ªØ k√≠
    def detect_signature(self, image):
        return self.signature_detection_model(image)

    # model nh·∫≠n di·ªán table title
    def detect_and_extract_title(self, image):

        if isinstance(image, np.ndarray):
            image = Image.fromarray(image.astype(np.uint8))

        
        # Nh·∫≠n di·ªán table trong ·∫£nh ƒë·ªÉ c·∫Øt ph·∫ßn title
        results = self.table_title_detection_model(image)

        # L·∫•y ·∫£nh g·ªëc
        img_last = results[0].orig_img.copy()

        # L·∫•y danh s√°ch t·ªça ƒë·ªô ch·ªØ k√Ω (x1, y1, x2, y2)
        boxes_obj = results[0].boxes
        if boxes_obj is not None and len(boxes_obj) > 0:
            coords = boxes_obj.xyxy.cpu().numpy()  # Chuy·ªÉn v·ªÅ numpy array
            x1, y1, x2, y2 = map(int, coords[0])  # L·∫•y t·ªça ƒë·ªô ƒë·∫ßu ti√™n (n·∫øu c√≥ nhi·ªÅu)

            # L·∫•y k√≠ch th∆∞·ªõc ·∫£nh
            h, w, _ = img_last.shape

            # C·∫Øt v√πng tr√™n v√† d∆∞·ªõi c·ªßa ch·ªØ k√Ω
            top_region = img_last[0:y1, 0:w]
            bottom_region = img_last[y2:h, x1:x2]

            # Nh·∫≠n di·ªán vƒÉn b·∫£n t·ª´ hai v√πng
            top_text = self.ocr.ocr(top_region)[0]
            bottom_text = self.ocr.ocr(bottom_region)[0]

            # L·ªçc k·∫øt qu·∫£ nh·∫≠n di·ªán
            top_result = [
                line[1][0]
                for line in (top_text or [])  # N·∫øu None th√¨ chuy·ªÉn th√†nh list r·ªóng
                if line and len(line) > 1 and line[1] and len(line[1]) > 0
            ]

            bottom_result = [
                line[1][0]
                for line in (bottom_text or [])  # N·∫øu None th√¨ chuy·ªÉn th√†nh list r·ªóng
                if line and len(line) > 1 and line[1] and len(line[1]) > 0
            ]

            # G·ªôp k·∫øt qu·∫£ t·ª´ c·∫£ hai v√πng
            extracted_text = top_result + bottom_result
        else:
            extracted_text = []
        df_title = pd.DataFrame(extracted_text)
        return df_title, extracted_text

    def normalize_text(self, text):
        return unidecode(str(text)).lower().strip()

    def recognize_financial_table(self, df, financial_tables, threshold=80):
        """
        Nh·∫≠n di·ªán ti√™u ƒë·ªÅ b·∫£ng t√†i ch√≠nh t·ª´ m·ªôt DataFrame.

        Args:
            df (pd.DataFrame): DataFrame ch·ª©a d·ªØ li·ªáu c·∫ßn ki·ªÉm tra.
            financial_tables (list): Danh s√°ch c√°c b·∫£ng t√†i ch√≠nh chu·∫©n.
            image : ·∫¢nh ƒëang x√©t
            threshold (int): Ng∆∞·ª°ng ƒë·ªô t∆∞∆°ng ƒë·ªìng t·ªëi thi·ªÉu ƒë·ªÉ ch·∫•p nh·∫≠n.
        Returns:
            tuple: (T√™n b·∫£ng t√†i ch√≠nh nh·∫≠n di·ªán ƒë∆∞·ª£c, ·∫£nh t∆∞∆°ng ·ª©ng)
        """
        # Chu·∫©n h√≥a danh s√°ch b·∫£ng t√†i ch√≠nh
        normalized_tables = [self.normalize_text(table) for table in financial_tables]

        # Duy·ªát qua t·ª´ng c·ªôt trong DataFrame
        for column in df.columns:
            for value in df[column].dropna():  # B·ªè qua gi√° tr·ªã NaN
                norm_value = self.normalize_text(value)

                # Ki·ªÉm tra kh·ªõp ch√≠nh x√°c tr∆∞·ªõc
                if norm_value in normalized_tables:
                    print(f"‚úÖ Kh·ªõp ch√≠nh x√°c: {value} (c·ªôt: {column})")
                    recognized_title = financial_tables[normalized_tables.index(norm_value)]
                    return recognized_title

                # N·∫øu kh√¥ng kh·ªõp ch√≠nh x√°c, ki·ªÉm tra ƒë·ªô t∆∞∆°ng ƒë·ªìng
                for norm_table in normalized_tables:
                    similarity = fuzz.partial_ratio(norm_value, norm_table)
                    if similarity >= threshold:
                        print(
                            f"üîπ Kh·ªõp t∆∞∆°ng ƒë·ªìng ({similarity}%): {value} ~ {norm_table} (c·ªôt: {column})"
                        )
                        recognized_title = financial_tables[
                            normalized_tables.index(norm_table)
                        ]
                        return recognized_title

        print("‚ùå Kh√¥ng t√¨m th·∫•y b·∫£ng t√†i ch√≠nh n√†o ph√π h·ª£p.")
        return None

    def get_model_params(self, model):
        if model == "gemini-2.0-pro-exp-02-05":
            return 1, 0.95, 64
        elif model == "gemini-2.0-flash-thinking-exp-01-21":
            return 0.7, 0.95, 64
        return None
